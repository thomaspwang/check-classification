#!./venv/bin/python3.10
""" Script for comparing predictions generated by analyze_checks.py and true labels. This script generates various aggregate statistics.

Usage:
    ./compare_predictions_to_labels.py <path_to_data_folder> <path_to_predictions.csv> <path_to_true_labels.csv>

Example:
    chmod +x compare_predictions_to_labels.py
    ./compare_predictions_to_labels.py ./data/mcd-test-3-front-images results.csv ./data/mcd-test-3-image-details.csv --verbose

Arguments:
    dataset_folder: Original input folder of check images (the same one fed to analyze_checks.py)
    predictions: .csv file generated by analyze_checks.py
    labels: True labels, originally a numbers file from @jts like mcd-test-3-image-details.numbers, but exported to csv
    verbose: This flag will print out every discrepancy & the check number associated with it (the ## in mcd-test-n-front-##.jpg)

This script generates various statistics: Average Edit Distance, Accuracy, and Hit Rate.
Hit rate refers to non-errors; when a strategy is unable to find a piece of data, it returns "NA". This is considered a non-hit.
Non-hits are nuanced, because they can be caused for multiple reasons; sometimes, non-hits are caused by missing check images;
some check images can be blank with only "Record of Deposit". Other times the non-hits are caused by read failures from the
tested strategies.

Edit Distance and Accuracy only apply to hits; non-hits are not considered in the calculation of these two statistics.

Accuracy is the percent of the time the prediction exactly matches the label.

Edit distance is the minimum amount of edits to go from the prediction to the label; e.g. 200.00 to 1200.00 is an edit
distance of 1. 20.08 to 20.00 is also an edit distance of one. The average edit distance is how many characters need to be
edited / deleted / added to get to the label. With the context of Accuracy, this is a good metric to estimate how "close"
our prediction are.

The original dataset_folder is an important argument in this script, as the only predictions that will be evaluated are on the
files present in dataset_folder. This allows users to select a subset of files from the original dataset without having to edit
the label file to only include the subset of labels.

e.g. dataset_folder contains a random 50% subset of the original mcd-test-n-front-images. The predictions .csv will contain each
prediction in check file number order. The label file will contain all the labels in check file number order. To properly compute
statistics, the script looks at the files present in the dataset_folder (and their check file numbers) to know which rows in the
label file to skip.
"""

import argparse
import csv
import os

def calculate_edit_distance(s1: str, s2: str) -> int:
    """This function calculates minimum edit distance.
    """
    # Initialize a 2D array to store edit distances
    dp = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]

    # Fill the first row and column
    for i in range(len(s1) + 1):
        dp[i][0] = i
    for j in range(len(s2) + 1):
        dp[0][j] = j

    # Calculate edit distance
    for i in range(1, len(s1) + 1):
        for j in range(1, len(s2) + 1):
            cost = 0 if s1[i - 1] == s2[j - 1] else 1
            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)

    return dp[-1][-1]

def all_na(row: List[str]) -> bool:
    """This function returns true if the row is composed entirely of "NA"

    This is used to sometimes skip rows, as a row of all "NA"s can mean that
    the check image is empty.
    """
    return all(value == "NA" for value in row.values())

# This removes columns in labels that are not present in predictions
def remove_extra_columns(predictions: Path, labels: Path) -> List[Dict[str, str]]:
    """This function removes columns not present in predictions.

    This helps get rid of extra columns, making it easier to debug.
    """
    with open(predictions, 'r') as file1, open(labels, 'r') as file2:
        reader1 = csv.reader(file1)
        reader2 = csv.reader(file2)
        headers1 = next(reader1)
        headers2 = next(reader2)

    columns_to_remove = [header for header in headers2 if header not in headers1]

    with open(labels, 'r') as file:
        reader = csv.DictReader(file)
        data2 = [{header: row[header] for header in headers2 if header not in columns_to_remove} for row in reader]

    # TODO: reorder columns in labels
    return data2

def remove_missing_rows(dataset_folder, labels):
    """This function removes rows in the label file which are not present in the dataset_folder.

    The dataset folder may only include a subset of the files listed in labels. This function
    removes the rows in labels whose corresponding files don't exist in the dataset folder.
    """
    def comparator(file_string):
        try:
            return int(file_string[17:-4])
        except:
            # arbitrary large number to kick weird files to the end.
            return 100000000

    files = os.listdir(dataset_folder)
    new_labels = []
    for file_name in sorted(files, key=comparator):
        try:
            new_labels.append(labels[int(file_name[17:-4])-1])
        except:
            pass
    return new_labels

def add_filename_column(labels):
    """This function adds a filename column to the labels.

    This helps users inspect which files had erroneous output.
    """
    for index in range(len(labels)):
        labels[index]["check_file_num"] = f"{index+1}"
    return labels


def calculate_statistics(dataset_folder, predictions, labels, verbose):
    """This function preprocesses the label file to match the predictions csv, then computes statistics.

    The label file needs to be preprocessed; the label file is a superset of the dataset_folder/predictions,
    so the extraneous rows need to be removed and filenames added.
    Once this is done, each row of the predictions now matches each row of the labels, and are compared.
    Aggregate statistics are then generated for each column in our prediction csv.

    If the dataset has many blank / "Record of Deposit" images, uncomment the if all_na snippet.
    This will skip any all "NA" rows, assuming that they were empty images.

    Any "Check Amount" column will recieve special treatment; dollar signs and commas will be removed
    to match the label file.
    """
    skip_idxs = []
    with open(predictions, 'r') as file:
        reader = csv.reader(file)
        headers = next(reader)

    with open(predictions, 'r') as file:
        reader = csv.DictReader(file)
        predictionData = [{header: row[header] for header in headers} for row in reader]

    labelData = remove_extra_columns(predictions, labels)

    labelData = add_filename_column(labelData)

    # skip rows of files not present in dataset. This only applies to partial datasets
    labelData = remove_missing_rows(dataset_folder, labelData)

    avg_edit_distance = {header: 0 for header in headers}
    counts = {header: 0 for header in headers}
    accuracy = {header: 0 for header in headers}
    missing_reads = {header: 0 for header in headers}
    hit_rate = {header: 0 for header in headers}

    total_rows = 0
    for index, (row1, row2) in enumerate(zip(predictionData, labelData)):

        # Uncomment this if the dataset you use has lots of blank / "Record Of Deposit" images.
        # However, some true negatives may be skipped by this snippet.
        # if all_na(row1):
        #     skip_idxs.append(index)
        #     continue

        total_rows +=1

        for header in headers:
            value1 = row1[header]
            value2 = row2[header]

            if value1.upper() == "NA":
                missing_reads[header] += 1
                if verbose:
                    print("MICR process error!")
                    print("Check file num: " + row2["check_file_num"])
                    print()
                continue

            # data cleaning
            value1 = value1.replace(" ", "")
            if header == "Check Amount":
                for char in CHARS_TO_REMOVE:
                    value1 = value1.replace(char, "")
                value1 = str(float(value1))
                value2 = str(float(value2))
            else:
                value1 = value1.upper()
                value2 = value2.upper()


            # compute edit distance
            counts[header]+=1
            edit_dist = calculate_edit_distance(value1, value2)
            avg_edit_distance[header] += edit_dist

            if calculate_edit_distance(value1, value2) == 0:
                accuracy[header]+=1

            # Print Errors
            if verbose and calculate_edit_distance(value1, value2) > 0:# and header == "Check Amount":
                print("Prediction, Label")
                print(value1, value2)
                print("Check file num: " + row2["check_file_num"])
                print()

    # Calculate the average edit distance for each column
    for header in headers:
        avg_edit_distance[header] /= counts[header]
        accuracy[header] /= counts[header]
        hit_rate[header] = counts[header] / (counts[header] + missing_reads[header])

    return avg_edit_distance, accuracy, hit_rate, missing_reads, total_rows, skip_idxs


if __name__ == "__main__":
    # Must be predictions, then labels; labels may be a superset of predictions.
    parser = argparse.ArgumentParser()
    parser.add_argument('dataset_folder', type=str, help='TODO')
    parser.add_argument('predictions', type=str, help='TODO')
    parser.add_argument('labels', type=str, help='TODO')
    parser.add_argument('--verbose', action='store_true', help='Set the flag to True')

    args = parser.parse_args()

    # We remove these characters as the label file does not have dollar signs or commas
    CHARS_TO_REMOVE = "$,"
    avg_edit_distance, accuracy, hit_rate, missing_reads, total_rows, skip_idxs = \
        calculate_statistics(args.dataset_folder, args.predictions, args.labels, args.verbose)
    print("Note: Edit distance and accuracy only account for hits.")
    print("Average Edit Distance for each column:")
    for header, distance in avg_edit_distance.items():
        print(f"{header}: {distance}")

    print("Accuracy for each column:")
    for header, acc in accuracy.items():
        print(f"{header}: {acc}")

    print("Hit Rate for each column:")
    for header, hit_rate in hit_rate.items():
        print(f"{header}: {hit_rate}")
